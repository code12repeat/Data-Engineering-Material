{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99eb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(\"/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark\")\n",
    "\n",
    "import pyspark\n",
    "\n",
    "sc= pyspark.SparkContext()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "                         .option(\"header\", \"true\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .load(\"/user/glbigdata12/Cars_Sale.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a894a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType, LongType, IntegerType, DateType\n",
    "\n",
    "# define the structure\n",
    "schema = StructType([\n",
    "    StructField(\"Manufacturer\", StringType(),True),\n",
    "    StructField(\"Model\", StringType(),True),\n",
    "    StructField(\"Vehicle_type\", StringType(),True),\n",
    "    StructField(\"Latest_Launch\", StringType(),True),\n",
    "    StructField(\"Units_Sold\", DoubleType(),True),\n",
    "    StructField(\"Units_Price\", DoubleType(),True),\n",
    "    StructField(\"Cost_incurred\", DoubleType(),True),\n",
    "    StructField(\"Revenue\", DoubleType(),True),\n",
    "    StructField(\"Cost\", DoubleType(),True),\n",
    "    StructField(\"Profit\", DoubleType(),True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# read the file by using the defined schema\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"/user/glbigdata12/Cars_Sale.csv\")\n",
    "\n",
    "# display the schema\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ab458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# select a few columns\n",
    "df1.select(\"Manufacturer\", \"Vehicle_type\", F.col(\"Model\"), \"Latest_Launch\", \"Units_Sold\", \"Revenue\", F.lit('DefaultValue')).show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select(\"*\").show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58295f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1deec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumnRenamed(\"Vehicle_type\", \"VehicleType\").withColumnRenamed('Units_Sold','units_sold').withColumnRenamed('Units_Price','units_price').withColumnRenamed('Cost_incurred','units_cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding columns to a dataframe\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# add a new column \"Register_Site\" with default value \"www.google.com\"\n",
    "dataDF = df1.withColumn(\"Register_Site\", F.lit(\"www.google.com\"))\n",
    "\n",
    "# display only a few columns\n",
    "dataDF.select(\"Manufacturer\", \"Vehicle_type\",\"Model\", \"Register_Site\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns from a DataFrame\n",
    "\n",
    "# number of columns in a dataframe - before removing columns\n",
    "print(\"Number of columns : \", len(dataDF.columns))\n",
    "\n",
    "# columns - before dropping\n",
    "print(list(dataDF.columns))\n",
    "\n",
    "# drop columns - \"Vehicle_type\", \"Model\"\n",
    "datanewDF = dataDF.drop(\"Vehicle_type\", \"Model\")\n",
    "\n",
    "# number of columns in a dataframe - after removing columns\n",
    "print(\"Number of columns : \", len(datanewDF.columns))\n",
    "\n",
    "# columns - after dropping\n",
    "print(list(datanewDF.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4cd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arithmetic with dataframes\n",
    "# number of columns in a dataframe - before a adding a column\n",
    "print(\"Number of columns : \", len(df1.columns))\n",
    "\n",
    "# perform arithmetic operations on a dataframe column\n",
    "newDF = df1.withColumn(\"TotalSale\", col(\"Units_Sold\") * col(\"Units_Price\"))\n",
    "\n",
    "# number of columns in a dataframe - after adding columns\n",
    "print(\"Number of columns : \", len(newDF.columns))\n",
    "\n",
    "# display records\n",
    "newDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter a dataframe\n",
    "\n",
    "df1.where(col(\"Manufacturer\") == \"Cadillac\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter a dataframe - multiple columns\n",
    "\n",
    "df1.where((col(\"Manufacturer\") == \"Cadillac\") & (col(\"Vehicle_type\") == \"Passenger\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7574eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows\n",
    "testDF = [[1, \"January\"], [2, \"February\"], [1, \"January\"], [3, \"March\"], [3, \"March\"], [3, \"March\"], [4, \"April\"], [4, \"April\"], [5, \"May\"], [5, \"May\"],\n",
    "          [4, \"April\"], [6, \"June\"], [5, \"April\"]]\n",
    "\n",
    "# import the modules\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# define the schema\n",
    "schema = StructType([StructField(\"ID\", IntegerType()),StructField(\"Month\", StringType())])\n",
    "\n",
    "# create the dataframe by applying schema\n",
    "df_new = spark.createDataFrame(testDF,schema=schema) \n",
    "\n",
    "# display the records\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display distinct rows\n",
    "df_new.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b42a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate records based a column value\n",
    "df_new.dropDuplicates(['Month']).show()\n",
    "\n",
    "# drop duplicate records based multiple column values\n",
    "df_new.dropDuplicates(['Month', 'ID']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12760de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename existing columns\n",
    "newDF1 = df1.withColumnRenamed(\"Units_Price\", \"UnitPrice\").withColumnRenamed(\"Profit\", \"Total_Profit\")\n",
    "\n",
    "df1.show(3) # display records\n",
    "\n",
    "from pyspark.sql.functions import expr # define the modules\n",
    "\n",
    "# using select expression \n",
    "newDF1.select(\"Manufacturer\", \"Model\",expr(\"CASE WHEN Total_Profit > 104 THEN  'Good' ELSE 'Average' END AS value_desc\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *   # import the libraries\n",
    "\n",
    "# define a list\n",
    "list_data = [[\"Bill Gates\",23],[\"Henry Ford\", None], [\"Tim Cook\", None]]\n",
    "\n",
    "# define the schema\n",
    "schema = StructType([StructField(\"Name\", StringType()),StructField(\"Experience\", IntegerType())])\n",
    "\n",
    "# create a dataframe \n",
    "df_new = spark.createDataFrame(list_data,schema=schema)\n",
    "\n",
    "df_new.show() # display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84625f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null value rows\n",
    "df_new.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec911c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null value with a constant value\n",
    "df_new.fillna(34).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73173c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace a single value\n",
    "df_new.na.replace('Bill Gates', 'Satya Nadella').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9735af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace multiple values and also fill 'null' with a constant value\n",
    "df_new.na.replace(['Bill Gates', 'Tim Cook'], ['Satya N', 'Time'], 'Name').fillna(40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d656256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the existing columns - \"Profit\" to \"Total_Profit\"\n",
    "newDF1 = df1.withColumnRenamed(\"Profit\", \"Total_Profit\")\n",
    "\n",
    "# find maximum total_profit for each region and alias the column to \"Maximum\"\n",
    "newDF1.groupBy(\"Manufacturer\").max(\"Total_Profit\").alias(\"Maximum\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of models by each manufacturer\n",
    "newDF1.groupBy(\"Manufacturer\").agg({'Model':'count'}).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg # include the library\n",
    "\n",
    "# find average of column - \"Total_Profit\" \n",
    "newDF1.select(avg(\"Total_Profit\").alias(\"Average Profit\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the library\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# order the records by manufacturer - ascending\n",
    "df1.orderBy('Manufacturer', ascending=True).select(\"Manufacturer\",\"Model\",\"Vehicle_type\", \"Profit\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the library\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# order the records by manufacturer - desc\n",
    "df1.orderBy('Manufacturer', ascending=False).select(\"Manufacturer\",\"Model\",\"Vehicle_type\", \"Profit\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650815ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache and persist\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# cache the dataframe in in-memory\n",
    "cacheDF = df1.cache()\n",
    "\n",
    "# read the records from cache\n",
    "cacheDF.select(\"Manufacturer\", \"Model\", \"Vehicle_type\",  \\\n",
    "               \"Latest_Launch\").show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af224c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache and persist\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# persist the dataframe in both memo\n",
    "persistDF = df1.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# read the records from saved dataframe\n",
    "persistDF.select(\"Manufacturer\", \"Model\", \"Vehicle_type\",  \\\n",
    "               \"Latest_Launch\").show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce vs repartition\n",
    "print(\"Number of partitions : \", df1.rdd.getNumPartitions())\n",
    "\n",
    "# increase the number of partitions\n",
    "cDF = df1.repartition(2)\n",
    "\n",
    "# number of partitions after repatitioning\n",
    "print(\"Number of partitions : \", cDF.rdd.getNumPartitions())\n",
    "\n",
    "# reduce the number of partitions\n",
    "cDF = cDF.coalesce(1)\n",
    "\n",
    "# number of partitions after coalesce\n",
    "print(\"Number of partitions : \", cDF.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates the Vehicle Type count by Manufacturer, brings the data to a single partition\n",
    "writeDF = newDF1.groupBy(\"Manufacturer\").agg({'Model':'count'}).coalesce(1)  \n",
    "\n",
    "# write to DBFS - mode: \"overwrite\" replaces the existing file and \"append\" adds the content\n",
    "writeDF.write.option(\"header\",\"true\").option(\"sep\",\",\").mode(\"overwrite\").csv(\"/user/glbigdata12/Aggregate/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%fs ls \"/user/glbigdata12/Aggregate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "newDF1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "   .load(\"/user/glbigdata12/Aggregate/part-00000-1de425a3-41cf-46d1-8fe8-c74b9d62149f-c000.csv\")\n",
    "\n",
    "# display the records\n",
    "newDF1.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43771b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark SQL\n",
    "# create a DataFrame\n",
    "from pyspark.sql.types import *   # import the library\n",
    "leader_data = [[\"Dodge\",\"Mohammed Saif\"],[\"Cadillac\", \"George Carlin\"], \\\n",
    "               [\"BMW\", \"Stuart Broad\"], [\"Ford\", \"Abdalla\"], [\"Hyundai\", \"Chris Gayle\"], \\\n",
    "               [\"Lexus\", \"George Bush\"], [\"Mercury\", \"Tatyaso Martin\"]]\n",
    "\n",
    "# define the schema\n",
    "schema = StructType([StructField(\"Manufacturer\", StringType()), StructField(\"SalesPerson\", StringType())])\n",
    "\n",
    "# create a dataframe and display the records\n",
    "df_new = spark.createDataFrame(leader_data,schema=schema)\n",
    "df_new.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.createOrReplaceTempView(\"sales_table\")  # convert dataframe to view\n",
    "\n",
    "# write sql queries using sql()\n",
    "spark.sql(\"select * from sales_table\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from sales_table where Manufacturer = 'Cadillac'\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45daad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from sales_table where SalesPerson like '%George%'\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from sales_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"vehicle\")\n",
    "\n",
    "spark.sql(\"select * from vehicle\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming a column using DSL\n",
    "newDF1 = df1.withColumnRenamed(\"Revenue\", \"TotalRevenue\")\n",
    "\n",
    "# create a temp view\n",
    "\n",
    "newDF1.createOrReplaceTempView(\"vehicle\")\n",
    "\n",
    "# apply aggregations on the table data\n",
    "spark.sql(\"select Manufacturer, max(TotalRevenue) from vehicle group by Manufacturer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select Manufacturer, max(TotalRevenue) from vehicle group by Manufacturer order by Manufacturer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select Manufacturer, max(TotalRevenue) from vehicle group by Manufacturer order by Manufacturer desc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a70799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join (inner) vehicle and sales_table, display the results\n",
    "spark.sql(\"\"\"select a.Manufacturer, a.Model, b.SalesPerson\n",
    "       from vehicle a\n",
    "       join sales_table b\n",
    "       on trim(a.Manufacturer) = trim(b.Manufacturer)\"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af97931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join (inner) vehicle and sales_table, apply a where condition, display the results\n",
    "df_new = spark.sql(\"\"\"select a.Manufacturer, a.Model, b.SalesPerson\n",
    "       from vehicle a\n",
    "       join sales_table b\n",
    "       on trim(a.Manufacturer) = trim(b.Manufacturer)\n",
    "       where trim(a.Manufacturer) = \"Cadillac\"\n",
    "       \"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b50f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results in to DBFS\n",
    "df_new = spark.sql(\"\"\"select a.Manufacturer, a.Model, b.SalesPerson\n",
    "       from vehicle a\n",
    "       join sales_table b\n",
    "       on trim(a.Manufacturer) = trim(b.Manufacturer)\n",
    "       where trim(a.Manufacturer) = \"Cadillac\"\n",
    "       \"\"\")\n",
    "\n",
    "\n",
    "df_new.coalesce(1).write.option(\"header\",\"true\").mode(\"overwrite\").csv(\"/user/glbigdata12/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da094051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%fs ls \"/user/glbigdata12/spark/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b2b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d8352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6da73ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a319d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36b877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7fe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16eaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8537c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31c9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72089d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa54ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958706a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a2812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
