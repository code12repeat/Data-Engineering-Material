{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ddb43a5-f7ae-4979-8ebe-776cc23d83a7",
   "metadata": {},
   "source": [
    "### 1. **Introduction to Sqoop**\n",
    "\n",
    "- **Purpose**: Sqoop is primarily used to import data from relational databases like MySQL, Oracle, PostgreSQL, and SQL Server into Hadoop's HDFS (Hadoop Distributed File System), Hive, or HBase. It can also export data from Hadoop back to these databases.\n",
    "- **Name Origin**: Sqoop stands for \"SQL-to-Hadoop.\"\n",
    "\n",
    "### 2. **Features of Sqoop**\n",
    "\n",
    "- **Full Load and Incremental Load**: Supports both full data loading and incremental loading (importing only new data).\n",
    "- **Parallel Import/Export**: Uses MapReduce to import/export data, allowing parallel operations for high performance.\n",
    "- **Data Compression**: Supports various compression techniques (e.g., gzip, bzip2) to reduce storage space and increase transfer speed.\n",
    "- **Data Format Support**: Supports various data formats, including text files, Avro, SequenceFiles, and Parquet.\n",
    "- **Integration with Hive/HBase**: Directly imports data into Hive tables or HBase.\n",
    "- **Security**: Supports Kerberos authentication and secure password storage.\n",
    "\n",
    "### 3. **Sqoop Architecture**\n",
    "\n",
    "Sqoop operates in the following steps:\n",
    "\n",
    "1. **Client-Side Commands**: Users issue Sqoop commands from the client-side interface.\n",
    "2. **Sqoop Driver**: The driver interprets these commands and manages the import/export jobs.\n",
    "3. **MapReduce Job**: Sqoop generates MapReduce jobs for importing/exporting data. \n",
    "4. **Data Source Connector**: Sqoop uses a connector (JDBC driver) specific to the database to interact with the source or target database.\n",
    "5. **HDFS/Hive/HBase**: Data is transferred between the source/target database and Hadoop storage components like HDFS, Hive, or HBase.\n",
    "\n",
    "### 4. **Common Sqoop Commands**\n",
    "\n",
    "- **Import Data from RDBMS to HDFS**:\n",
    "  ```bash\n",
    "  sqoop import --connect jdbc:mysql://localhost:3306/database_name --username root --password password --table table_name --target-dir /user/hdfs/target_dir\n",
    "  ```\n",
    "  - `--connect`: JDBC connection string for the database.\n",
    "  - `--username` and `--password`: Database credentials.\n",
    "  - `--table`: Name of the table to import.\n",
    "  - `--target-dir`: HDFS directory where data will be stored.\n",
    "\n",
    "- **Import Data from RDBMS to Hive**:\n",
    "  ```bash\n",
    "  sqoop import --connect jdbc:mysql://localhost:3306/database_name --username root --password password --table table_name --hive-import --create-hive-table --hive-table hive_db.hive_table\n",
    "  ```\n",
    "  - `--hive-import`: Directly imports data into a Hive table.\n",
    "  - `--create-hive-table`: Creates the Hive table if it doesn't exist.\n",
    "  - `--hive-table`: Specifies the target Hive table.\n",
    "\n",
    "- **Import Data from RDBMS to HBase**:\n",
    "  ```bash\n",
    "  sqoop import --connect jdbc:mysql://localhost:3306/database_name --username root --password password --table table_name --hbase-table hbase_table --column-family column_family\n",
    "  ```\n",
    "  - `--hbase-table`: Name of the HBase table where data will be imported.\n",
    "  - `--column-family`: Column family in HBase.\n",
    "\n",
    "- **Incremental Import**:\n",
    "  ```bash\n",
    "  sqoop import --connect jdbc:mysql://localhost:3306/database_name --username root --password password --table table_name --incremental append --check-column id --last-value 100\n",
    "  ```\n",
    "  - `--incremental append`: Performs incremental import by appending new data.\n",
    "  - `--check-column`: Column used to check for new data.\n",
    "  - `--last-value`: Last imported value for the `--check-column`.\n",
    "\n",
    "- **Export Data from HDFS to RDBMS**:\n",
    "  ```bash\n",
    "  sqoop export --connect jdbc:mysql://localhost:3306/database_name --username root --password password --table table_name --export-dir /user/hdfs/target_dir\n",
    "  ```\n",
    "  - `--export-dir`: HDFS directory containing data to export.\n",
    "\n",
    "### 5. **Sqoop Connectors**\n",
    "\n",
    "- **JDBC Connector**: Most databases support JDBC (Java Database Connectivity), which Sqoop uses to connect to various databases.\n",
    "- **Specific Connectors**: Sqoop has specific connectors optimized for certain databases like MySQL, PostgreSQL, Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb39c55-7c6b-4636-86bd-a52bb0091976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Employee Data:\n",
      "   employee_id first_name last_name  age   department  salary\n",
      "0          101       John       Doe   28        Sales   50000\n",
      "1          102       Jane     Smith   34           HR   60000\n",
      "2          103       Mike   Johnson   29  Engineering   75000\n",
      "3          104       Sara     Brown   42    Marketing   85000\n",
      "Data successfully inserted into the 'empTable' table.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a sample employee DataFrame\n",
    "data = {\n",
    "    'employee_id': [101, 102, 103, 104],\n",
    "    'first_name': ['John', 'Jane', 'Mike', 'Sara'],\n",
    "    'last_name': ['Doe', 'Smith', 'Johnson', 'Brown'],\n",
    "    'age': [28, 34, 29, 42],\n",
    "    'department': ['Sales', 'HR', 'Engineering', 'Marketing'],\n",
    "    'salary': [50000, 60000, 75000, 85000]\n",
    "}\n",
    "\n",
    "employee_df = pd.DataFrame(data)\n",
    "print(\"Sample Employee Data:\")\n",
    "print(employee_df)\n",
    "\n",
    "# Define the database connection parameters\n",
    "db_user = 'maneelcha49dgre'\n",
    "db_password = 'BrownGorilla19$'\n",
    "db_host = 'master'  # or your MySQL server address\n",
    "db_port = '3306'       # default MySQL port\n",
    "db_name = 'maneelcha49dgre'\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(f'mysql+mysqlconnector://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# Send the DataFrame to the MySQL database\n",
    "table_name = 'empTable'\n",
    "\n",
    "try:\n",
    "    employee_df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)#send table to hive\n",
    "    print(f\"Data successfully inserted into the '{table_name}' table.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29722136-8a3c-4232-87f0-df70e8aa3336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>department</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>28</td>\n",
       "      <td>Sales</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>34</td>\n",
       "      <td>HR</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Mike</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>29</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Sara</td>\n",
       "      <td>Brown</td>\n",
       "      <td>42</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>85000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_id first_name last_name  age   department  salary\n",
       "0          101       John       Doe   28        Sales   50000\n",
       "1          102       Jane     Smith   34           HR   60000\n",
       "2          103       Mike   Johnson   29  Engineering   75000\n",
       "3          104       Sara     Brown   42    Marketing   85000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql('select * from empTable ' , engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82bd88a-9a0c-4981-a381-99cd731380c0",
   "metadata": {},
   "source": [
    "# Execute this command in web shell\n",
    "\n",
    "### change username , password and table name\n",
    "\n",
    "### sqoop import --connect jdbc:mysql://master:3306/ankit810248bgre --username ankit810248bgre --password SilverOwl38$ --table employees_1 --hive\n",
    "### -import --create-hive-table --hive-table ankit.employees_1 -m 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae933d-7eb1-4652-9364-e3d7589026e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
