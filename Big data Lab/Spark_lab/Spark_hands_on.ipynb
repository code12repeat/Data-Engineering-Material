{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5541b03a-d4db-4bed-9b33-8f241d9732a1",
   "metadata": {},
   "source": [
    "### Part1 : Basic/Easy RDD Operations\n",
    "### Write Spark RDD commands for the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ed24a4-0c0e-48eb-a047-294971be1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext is for RDDs\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1f54e6-26f4-4c27-bffa-4b21ee0d641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 15:28:43,034 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#initialize sparkcontext\n",
    "sc=SparkContext('local','wordCount')\n",
    "data=[1,2,3,4,5]\n",
    "#1.To load the given list into a spark RDD, list1 = [1,2,3,4,5].\n",
    "rdd=sc.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "770a8783-5409-42db-afa2-2a4755d4b7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#2.Display/Print first four elements of the RDD\n",
    "print(rdd.take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a900c6-42b0-44dd-a8d5-80e9fb323a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#3.Display/Print the first element of the RDD\n",
    "print(rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8366c23f-811a-4fc3-b5d3-fdf5f9d0042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#4.Display/Print the number of elements in the RDD.\n",
    "count_num=rdd.count()\n",
    "print(count_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab167dd-b9e0-4b23-85a8-87b201892be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "#5. Display sum of all elements of the RDD.\n",
    "sum=rdd.sum()\n",
    "print(sum)\n",
    "#or\n",
    "total_sum=rdd.reduce(lambda x,y:x+y)\n",
    "print(total_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94458c7e-9005-4a87-bded-a8d0858a28ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#6. Display maximum value of the RDD.\n",
    "max_value=rdd.max()\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81e42411-6907-44c1-a2b8-39c73b7e09b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#7. Display minimum value of the RDD.\n",
    "min_value=rdd.min()\n",
    "print(min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f11487a-7e64-430c-b795-28ff8aa2b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#8. Display the first 5 element of the RDD.\n",
    "print(rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b201f5f-fae5-469f-9408-5a7b45307751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#9. Display the first element of the RDD.\n",
    "first_el=rdd.take(1)\n",
    "print(first_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0349db2-c22d-4fa6-a499-e656ab29a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "#10. Display the mean value among all elements of the RDD.\n",
    "mean_val=rdd.mean()\n",
    "print(mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a9265-7052-4065-8cf3-07da15c5ec47",
   "metadata": {},
   "source": [
    "### Part 2 : Moderate RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d062b84-c1ec-447b-9c42-10807a8a5908",
   "metadata": {},
   "source": [
    "### 1. Write Spark RDD transformations to square each element of the RDD ( used in above part 1 )  and return a new RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03a663d8-25f0-4bc3-93c8-2945ec36d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "rdd_1=rdd.map(lambda x:x*x)\n",
    "print(rdd_1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f40e2-2258-4d25-9972-8404b30e0d9b",
   "metadata": {},
   "source": [
    "### 2. Given two RDDs rdd1 and rdd2, containing integers, write a Spark RDD transformation to find the union of the two RDDs, removing any duplicate elements, and return a new RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fed4a797-59b4-480f-b98a-ce22a86a0e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 16, 1, 3, 5, 9, 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_2=rdd.union(rdd_1).distinct()\n",
    "print(rdd_2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d8993-6845-4f3c-85ec-0ae163fca117",
   "metadata": {},
   "source": [
    "### 3. Consider an RDD rdd containing key-value pairs. Write a Spark RDD transformation to filter the RDD and return a new RDD with only the key-value pairs where the value is greater than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "976d488f-69fc-4cf3-8894-f47da24e8eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 12), (3, 11)]\n"
     ]
    }
   ],
   "source": [
    "sample_data=[\n",
    "    (1,4),(2,12),(3,11),(4,9)\n",
    "]\n",
    "rdd_3=sc.parallelize(sample_data)\n",
    "# print(rdd_3.collect())\n",
    "res=rdd_3.filter(lambda x:x[1]>10)\n",
    "print(res.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f4e8a-57e2-40b9-b66b-093d6036da1c",
   "metadata": {},
   "source": [
    "### 4. Given an RDD rdd containing strings, write a Spark RDD transformation to count the occurrences of each unique string and return a new RDD with key-value pairs, where the key is the string and the value is its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d19f69b6-0ab2-4185-932b-02bf35d394eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "[('hello', 2), ('hi', 2), ('spark', 2), ('world', 1)]\n"
     ]
    }
   ],
   "source": [
    "st=[\"hello\",\"hi\",\"spark\",\"hello\",\"hi\",\"spark\",\"world\"]\n",
    "rdd_4=sc.parallelize(st)\n",
    "# print(rdd_4.collect())\n",
    "new_rdd=rdd_4.map(lambda x:(x,1))\n",
    "ans=new_rdd.reduceByKey(lambda x,y:x+y)\n",
    "print(ans.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9351e-7014-43fc-bfb9-88d64c85b684",
   "metadata": {},
   "source": [
    "### 5. Given an RDD rdd containing integers, write a Spark RDD action to calculate the sum of all the elements in the RDD and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffe7d4ac-ec7b-47f6-9e91-4d542d205439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(rdd_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d9715-b7d5-4855-82e4-a403567200a2",
   "metadata": {},
   "source": [
    "### Part 3: Spark Use Case: Log analysis using Spark RDD\n",
    "#### Write a Spark (PySpark) program using RDD operations to perform the following tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e90c4f8-d553-454c-adbf-ff986d044424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdfs dfs -put apache_logs.txt /user/maneelcha49dgre (to upload data on hdfs in terminal)\n",
    "#OR\n",
    "import os\n",
    "os.environ['HADOOP_HOME'] = '/opt/hadoop' \n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "local_file = \"apache_logs.txt\"\n",
    "hdfs_directory = \"/user/maneelcha49dgre/\"\n",
    "!hdfs dfs -put {local_file} {hdfs_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae653f-dd39-4609-ab81-296880b70577",
   "metadata": {},
   "source": [
    "### a. Read the log file from provided log file ‘apache_logs.txt’ and create an RDD with each line of the log file as an element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "981101a7-c6f3-4065-a372-80598020be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import re\n",
    "rdd_5=sc.textFile('apache_logs.txt')\n",
    "# print(rdd_5.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e3f87-e409-4c1a-b148-7f770ce4fe26",
   "metadata": {},
   "source": [
    "### b. Filter the RDD to keep only the lines that contain the string \"ERROR\" (case-insensitive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c25acbd1-f6b8-4363-b608-0d95648f3a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['66.249.73.135 - - [18/May/2015:05:05:49 +0000] \"GET /blog/geekery/ie-javascript-error.html HTTP/1.1\" 200 8600 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"', '66.249.73.135 - - [18/May/2015:05:05:08 +0000] \"GET /blog/geekery/vim-function-to-make-errors-readable.html HTTP/1.1\" 200 10260 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"', '207.241.237.224 - - [18/May/2015:13:05:45 +0000] \"GET /blog/geekery/vim-function-to-make-errors-readable.html HTTP/1.0\" 200 10260 \"http://www.semicomplete.com/blog/tags/year%20review\" \"Mozilla/5.0 (compatible; archive.org_bot +http://www.archive.org/details/archive.org_bot)\"', '66.249.73.135 - - [18/May/2015:16:05:31 +0000] \"GET /misc/errors.m4 HTTP/1.1\" 200 7420 \"-\" \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"', '208.43.252.200 - - [18/May/2015:21:05:56 +0000] \"GET /blog/geekery/vim-function-to-make-errors-readable.html HTTP/1.1\" 200 10260 \"-\" \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/19.0.1084.52)\"', '208.43.251.181 - - [19/May/2015:07:05:16 +0000] \"GET /blog/geekery/vim-function-to-make-errors-readable.html HTTP/1.1\" 200 10260 \"-\" \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/19.0.1084.52)\"', '125.122.211.40 - - [20/May/2015:04:05:26 +0000] \"GET /articles/dynamic-dns-with-dhcp/ HTTP/1.1\" 200 18848 \"http://www.baidu.com/s?ie=utf-8&bs=60.191.124.236&f=3&rsv_bp=1&rsv_spt=3&wd=TSIG+error+with+server%3A+tsig+indicates+error&rsv_sug3=2&rsv_sug4=31&rsv_sug1=2&rsp=0&inputT=0\" \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101 Firefox/25.0\"']\n"
     ]
    }
   ],
   "source": [
    "filtered_rdd = rdd_5.filter(lambda line: \"ERROR\" in line.upper())\n",
    "print(filtered_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3288a7-a6b3-4660-b802-60806dc617e7",
   "metadata": {},
   "source": [
    "### c. Extract the IP address from each error line. The IP address can be extracted by parsing the line and extracting the substring that represents the IP address\n",
    "### d. Count the occurrences of each unique IP address and return an RDD with tuples containing the IP address and its count.\n",
    "### e. Print the IP address and its corresponding count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "533695de-e0cb-410c-b4f9-60087b270853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.249.73.135: 3\n",
      "207.241.237.224: 1\n",
      "208.43.252.200: 1\n",
      "208.43.251.181: 1\n",
      "125.122.211.40: 1\n"
     ]
    }
   ],
   "source": [
    "def extract_ip(line):\n",
    "    ip_pattern = r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'\n",
    "    match = re.search(ip_pattern, line)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "ip_rdd = filtered_rdd.map(extract_ip).filter(lambda ip: ip is not None)\n",
    "ip_count_rdd = ip_rdd.map(lambda ip: (ip, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "for ip, count in ip_count_rdd.collect():\n",
    "    print(f\"{ip}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a49a9-7e0c-48f0-b918-8c4044de261b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
