{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcd5de2-fb78-4032-a52e-1815ec03fde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "2024-09-05 10:38:41,158 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-09-05 10:38:42,022 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2024-09-05 10:38:42,022 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2024-09-05 10:38:42,022 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "2024-09-05 10:38:42,023 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "2024-09-05 10:38:42,023 WARN util.Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "2024-09-05 10:38:42,023 WARN util.Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "2024-09-05 10:38:42,023 WARN util.Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "2024-09-05 10:38:42,023 WARN util.Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "2024-09-05 10:38:42,023 WARN util.Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "2024-09-05 10:38:42,024 WARN util.Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "2024-09-05 10:38:42,024 WARN util.Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "2024-09-05 10:38:42,780 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Initialize SparkSession with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HiveSelectQuery\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    " \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1650ec-4c47-4728-8db1-ceedffd6baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists fireBase\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54c9454-21f2-4af0-af3b-85670871c0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         namespace|\n",
      "+------------------+\n",
      "|            01sept|\n",
      "|    1sept2024_anup|\n",
      "|         24aug2024|\n",
      "|    27082024_shyam|\n",
      "|27_aug_2024_devang|\n",
      "| 27_aug_2024_pavan|\n",
      "| 27_aug_2024_prawn|\n",
      "|27_aug_2024_sumedh|\n",
      "|     27_aug_chirag|\n",
      "|         27aug2024|\n",
      "|  27aug2024_aniket|\n",
      "|    27aug2024_kuru|\n",
      "|      27aug2024_m1|\n",
      "|     27aug2024_man|\n",
      "| 27aug2024_nishant|\n",
      "|  27aug2024_rajish|\n",
      "|  27aug2024_rakesh|\n",
      "|      27aug2024_rs|\n",
      "| 27aug2024_srirang|\n",
      "|      27aug2024_ss|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee37b52b-7d88-40c8-9e68-0aebfbc73153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use fireBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b25313-d78d-45dd-8ed6-7d7e058f1806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|name                        |string                                                        |null   |\n",
      "|age                         |int                                                           |null   |\n",
      "|salary                      |double                                                        |null   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Database                    |firebase                                                      |       |\n",
      "|Table                       |emp                                                           |       |\n",
      "|Owner                       |maneelcha49dgre                                               |       |\n",
      "|Created Time                |Thu Sep 05 04:54:55 UTC 2024                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.1.2                                                   |       |\n",
      "|Type                        |EXTERNAL                                                      |       |\n",
      "|Provider                    |parquet                                                       |       |\n",
      "|Location                    |hdfs:/user/maneelcha49dgre/Spark/fireBase                     |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate the table with the desired format\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE emp (\n",
    "        name STRING,\n",
    "        age INT,\n",
    "        salary DOUBLE\n",
    "    )\n",
    "    USING parquet\n",
    "    LOCATION 'hdfs:////user/maneelcha49dgre/Spark/fireBase'\n",
    "\"\"\")\n",
    "\n",
    "# # Check the table properties to see the existing format\n",
    "spark.sql(\"DESCRIBE FORMATTED emp\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bfea63-7566-4aa2-8b9a-53dc64260919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+\n",
      "|name|age|salary|\n",
      "+----+---+------+\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from emp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a79cd38c-93a1-4b86-b844-da88e3b1f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into table emp values('Max',23,4324.3),('Tony',21,234567.0),('John',34,3456.7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212256ab-17dc-4646-9902-6af50438d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   name|age|  salary|\n",
      "+-------+---+--------+\n",
      "|  Alice| 30|  3000.0|\n",
      "|    Bob| 25|  2500.0|\n",
      "|Charlie| 35|  3500.0|\n",
      "|   Tony| 21|234567.0|\n",
      "|   John| 34|  3456.7|\n",
      "|    Max| 23|  4324.3|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from emp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767a20a3-4e0d-4d57-a587-394bf22bf9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  Alice| 30|3000.0|\n",
      "|    Bob| 25|2500.0|\n",
      "|Charlie| 35|3500.0|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# Step 1: Create a simple Spark DataFrame\n",
    "data = [\n",
    "    Row(name=\"Alice\", age=30, salary=3000.0),\n",
    "    Row(name=\"Bob\", age=25, salary=2500.0),\n",
    "    Row(name=\"Charlie\", age=35, salary=3500.0)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1637fe16-51fa-4a01-9f7c-2e55d3e62396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Insert data into the Hive table\n",
    "df.write.mode(\"append\").saveAsTable(\"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703eb5c1-e0d0-411c-bd3c-660810724d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
