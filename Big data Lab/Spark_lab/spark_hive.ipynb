{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3f4f28-8f31-49b8-a317-9324d82f537d",
   "metadata": {},
   "source": [
    "\n",
    "# Create a schema\n",
    "#### Step 1: Create a SparkSession\n",
    "\n",
    "First, initialize the Spark session. This session serves as the entry point for reading and manipulating data in Spark.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadCSVWithSchema\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "#### Step 2: Define the Schema\n",
    "\n",
    "Define a schema for the CSV file using `StructType` and `StructField`. This schema explicitly specifies the data types for each column.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define the schema for the CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),  # Column \"Name\" with data type String\n",
    "    StructField(\"Age\", IntegerType(), True),  # Column \"Age\" with data type Integer\n",
    "    StructField(\"Salary\", DoubleType(), True) # Column \"Salary\" with data type Double\n",
    "])\n",
    "```\n",
    "\n",
    "#### Step 3: Read the CSV File with the Defined Schema\n",
    "\n",
    "Use the `spark.read.csv()` method to read the CSV file into a DataFrame, passing the schema as an argument.\n",
    "\n",
    "```python\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"path/to/your/file.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame using the predefined schema\n",
    "df = spark.read.csv(csv_file_path, header=True, schema=schema)\n",
    "\n",
    "# Show the DataFrame content (for demonstration)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "- **`header=True`**: Indicates that the first line of the CSV file contains column headers.\n",
    "- **`schema=schema`**: Specifies the schema defined earlier, which Spark will use to parse the CSV data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c67bd0-aea7-40f1-a51e-8b7ba9e401d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize SparkSession with Hive support\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"HiveConnectionCheck\") \\\n",
    "#     .config(\"spark.sql.warehouse.dir\",\"/user/hive/warehouse\")\\\n",
    "#     .config(\"hive.metastore.uris\",\"thrift://localhost:9083\")\\\n",
    "#     .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73490f48-2823-4eb5-b9f2-f71bd756a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HiveConnectionCheck\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd608138-af09-49d2-b30d-0c9c5e3ceb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all tables in the 'default' database\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e24d785e-91c0-4def-bbd4-f360c546d591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|name                        |string                                                        |null   |\n",
      "|age                         |int                                                           |null   |\n",
      "|salary                      |double                                                        |null   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Database                    |spark_db                                                      |       |\n",
      "|Table                       |emp                                                           |       |\n",
      "|Owner                       |maneelcha49dgre                                               |       |\n",
      "|Created Time                |Wed Sep 04 04:56:49 UTC 2024                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.1.2                                                   |       |\n",
      "|Type                        |EXTERNAL                                                      |       |\n",
      "|Provider                    |parquet                                                       |       |\n",
      "|Location                    |hdfs:/user/maneelcha49dgre/Spark/spark_db                     |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the existing table if it exists\n",
    "spark.sql(\"create database spark_db\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS spark_db.emp\")\n",
    "# Recreate the table with the desired format\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE spark_db.emp (\n",
    "        name STRING,\n",
    "        age INT,\n",
    "        salary DOUBLE\n",
    "    )\n",
    "    USING parquet\n",
    "    LOCATION 'hdfs:////user/maneelcha49dgre/Spark/spark_db'\n",
    "\"\"\")\n",
    "\n",
    "# # Check the table properties to see the existing format\n",
    "spark.sql(\"DESCRIBE FORMATTED spark_db.emp\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40415c16-4b08-47d0-b519-daf57979165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb60f6-2730-4c4b-a1ee-1c7c176f70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02314917-1dd4-4b6a-9ac2-9560323c9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from emp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b42a2cc-05ee-4bfd-af25-c06783e23368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  Alice| 30|3000.0|\n",
      "|    Bob| 25|2500.0|\n",
      "|Charlie| 35|3500.0|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# Step 1: Create a simple Spark DataFrame\n",
    "data = [\n",
    "    Row(name=\"Alice\", age=30, salary=3000.0),\n",
    "    Row(name=\"Bob\", age=25, salary=2500.0),\n",
    "    Row(name=\"Charlie\", age=35, salary=3500.0)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12dbc0d0-a8bc-45d8-b676-b82f2908b933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 4: Insert data into the Hive table\n",
    "df.write.mode(\"append\").saveAsTable(\"spark_db.emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1432152-d4c7-46bc-861c-99754ee3a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  Alice| 30|3000.0|\n",
      "|    Bob| 25|2500.0|\n",
      "|Charlie| 35|3500.0|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the data by querying the Hive table\n",
    "result_df = spark.sql(\"SELECT * FROM spark_db.emp\")\n",
    "result_df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3795848c-1571-4492-b2d7-0a66394a947b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#insert data into spark_db.emp table\n",
    "spark.sql(\"insert into table spark_db.emp values('Max',34,34000.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b9f2642-c892-4a8b-90e2-a415e8d6b8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|   name|age| salary|\n",
      "+-------+---+-------+\n",
      "|  Alice| 30| 3000.0|\n",
      "|    Bob| 25| 2500.0|\n",
      "|Charlie| 35| 3500.0|\n",
      "|    Max| 34|34000.0|\n",
      "+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\"SELECT * FROM spark_db.emp\")\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
