{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a180769-4b81-4c3a-9fd0-297a4d42041b",
   "metadata": {},
   "source": [
    "#### 1. **Set the Schema and Load Data using Structured Streaming (5 Marks)**\n",
    "   - **Objective**: Load data from different locations into data frames using PySpark's structured streaming.\n",
    "   - **Tools**: PySpark, HDFS, Kafka.\n",
    "   - **Approach**:\n",
    "     - **HDFS**: Store all datasets (`cust_dimen`, `market_fact`, `orders_dimen`, `prod_dimen`, `shipping_dimen`) in HDFS.\n",
    "     - **Kafka**: Stream data into Spark. Use Kafka topics for each dataset and set up producers to publish data from HDFS to Kafka.\n",
    "     - **PySpark Structured Streaming**: Read data from Kafka topics using structured streaming and define schemas for each dataset.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       from pyspark.sql import SparkSession\n",
    "       from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "       \n",
    "       spark = SparkSession.builder \\\n",
    "           .appName(\"SalesDataStreaming\") \\\n",
    "           .getOrCreate()\n",
    "\n",
    "       schema = StructType([\n",
    "           StructField(\"Column1\", StringType(), True),\n",
    "           StructField(\"Column2\", IntegerType(), True),\n",
    "           # Add other fields based on your datasets\n",
    "       ])\n",
    "       \n",
    "       df = spark.readStream \\\n",
    "           .format(\"kafka\") \\\n",
    "           .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "           .option(\"subscribe\", \"cust_dimen\") \\\n",
    "           .load()\n",
    "       \n",
    "       df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "       df = spark.read.json(df.rdd, schema)\n",
    "       ```\n",
    "\n",
    "#### 2. **Join DataFrames to Create `Full_DataFrame` (5 Marks)**\n",
    "   - **Objective**: Combine all datasets into a single DataFrame without duplicate columns.\n",
    "   - **Approach**:\n",
    "     - Use PySpark DataFrame join operations.\n",
    "     - Ensure you remove or rename duplicate columns during the join process.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       full_df = cust_dimen_df.join(market_fact_df, \"common_column\") \\\n",
    "                              .join(orders_dimen_df, \"common_column\") \\\n",
    "                              .join(prod_dimen_df, \"common_column\") \\\n",
    "                              .join(shipping_dimen_df, \"common_column\")\n",
    "       ```\n",
    "\n",
    "#### 3. **Convert Date Columns and Show Top 5 Records (5 Marks)**\n",
    "   - **Objective**: Convert `Order_Date` and `Ship_Date` columns to `DateType` and display schema and records.\n",
    "   - **Approach**:\n",
    "     - Use `to_date` function in PySpark to convert the columns.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       from pyspark.sql.functions import to_date\n",
    "       \n",
    "       full_df = full_df.withColumn(\"Order_Date\", to_date(full_df[\"Order_Date\"], \"yyyy-MM-dd\")) \\\n",
    "                        .withColumn(\"Ship_Date\", to_date(full_df[\"Ship_Date\"], \"yyyy-MM-dd\"))\n",
    "       \n",
    "       full_df.printSchema()\n",
    "       full_df.select(\"Order_Date\", \"Ship_Date\").show(5)\n",
    "       ```\n",
    "\n",
    "#### 4. **Find Top 3 Customers by Number of Orders (5 Marks)**\n",
    "   - **Objective**: Identify the top 3 customers with the highest number of orders.\n",
    "   - **Approach**:\n",
    "     - Group by `customer_id` and count orders, then use `orderBy` to get the top 3.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       top_customers = full_df.groupBy(\"customer_id\").count().orderBy(\"count\", ascending=False).limit(3)\n",
    "       top_customers.show()\n",
    "       ```\n",
    "\n",
    "#### 5. **Create `DaysTakenForDelivery` Column (5 Marks)**\n",
    "   - **Objective**: Calculate the difference between `Order_Date` and `Ship_Date`.\n",
    "   - **Approach**:\n",
    "     - Use the `datediff` function.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       from pyspark.sql.functions import datediff\n",
    "       \n",
    "       full_df = full_df.withColumn(\"DaysTakenForDelivery\", datediff(\"Ship_Date\", \"Order_Date\"))\n",
    "       full_df.show(5)\n",
    "       ```\n",
    "\n",
    "#### 6. **Find Customer with Maximum Delivery Time (5 Marks)**\n",
    "   - **Objective**: Identify the customer whose order took the longest to deliver.\n",
    "   - **Approach**:\n",
    "     - Use `orderBy` on the `DaysTakenForDelivery` column.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       max_delivery_customer = full_df.orderBy(\"DaysTakenForDelivery\", ascending=False).select(\"customer_id\").first()\n",
    "       print(max_delivery_customer)\n",
    "       ```\n",
    "\n",
    "#### 7. **Retrieve Total Sales per Product using Window Functions (5 Marks)**\n",
    "   - **Objective**: Calculate total sales for each product.\n",
    "   - **Approach**:\n",
    "     - Use PySpark Window functions to partition by `product_id`.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       from pyspark.sql.window import Window\n",
    "       from pyspark.sql.functions import sum as _sum\n",
    "       \n",
    "       windowSpec = Window.partitionBy(\"product_id\")\n",
    "       sales_per_product = full_df.withColumn(\"Total_Sales\", _sum(\"sales\").over(windowSpec))\n",
    "       sales_per_product.show()\n",
    "       ```\n",
    "\n",
    "#### 8. **Retrieve Total Profit per Product (5 Marks)**\n",
    "   - **Objective**: Calculate total profit per product using and without using window functions.\n",
    "   - **Approach**:\n",
    "     - **With Window Function**:\n",
    "       ```python\n",
    "       profit_per_product = full_df.withColumn(\"Total_Profit\", _sum(\"profit\").over(windowSpec))\n",
    "       profit_per_product.show()\n",
    "       ```\n",
    "     - **Without Window Function**:\n",
    "       ```python\n",
    "       profit_per_product_no_window = full_df.groupBy(\"product_id\").sum(\"profit\")\n",
    "       profit_per_product_no_window.show()\n",
    "       ```\n",
    "\n",
    "#### 9. **Count Unique Customers in January and Recurring Customers (5 Marks)**\n",
    "   - **Objective**: Count unique customers in January 2011 and those who return every month throughout the year.\n",
    "   - **Approach**:\n",
    "     - Use `filter` for January data and `groupBy` for counting returning customers.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       from pyspark.sql.functions import month, year\n",
    "       \n",
    "       jan_customers = full_df.filter((month(\"Order_Date\") == 1) & (year(\"Order_Date\") == 2011)).select(\"customer_id\").distinct()\n",
    "       returning_customers = full_df.groupBy(\"customer_id\").agg(_countDistinct(\"Order_Date\").alias(\"order_months\"))\n",
    "       recurring_customers = returning_customers.filter(\"order_months = 12\")\n",
    "       jan_customers_count = jan_customers.count()\n",
    "       recurring_customers_count = recurring_customers.count()\n",
    "       ```\n",
    "\n",
    "#### 10. **Calculate Total Sales, Profit, Quantity, and Discount by Customer (5 Marks)**\n",
    "   - **Objective**: Compute total sales, profit, quantity, and discounts by customer and sort by total profit.\n",
    "   - **Approach**:\n",
    "     - Use `groupBy` and aggregation functions.\n",
    "     - Example code snippet:\n",
    "       ```python\n",
    "       result_df = full_df.groupBy(\"customer_id\") \\\n",
    "           .agg(_sum(\"quantity\").alias(\"Total_Quantity\"),\n",
    "                _sum(\"discount\").alias(\"Total_Discount\"),\n",
    "                _sum(\"sales\").alias(\"Total_Sales\"),\n",
    "                _sum(\"profit\").alias(\"Total_Profit\")) \\\n",
    "           .orderBy(\"Total_Profit\", ascending=False)\n",
    "       \n",
    "       result_df.show()\n",
    "       ```\n",
    "\n",
    "### Additional Steps\n",
    "\n",
    "1. **Data Loading with Sqoop**:\n",
    "   - If the data is stored in a relational database, use Sqoop to import the data into HDFS.\n",
    "\n",
    "2. **Data Storage in Hive**:\n",
    "   - Create external Hive tables pointing to the data stored in HDFS.\n",
    "   - This allows you to run SQL queries using PySpark's `spark.sql` method for additional analysis.\n",
    "\n",
    "3. **Setting Up the Environment in Jupyter**:\n",
    "   - Ensure all necessary PySpark configurations are set in your Jupyter notebook, and all required libraries are imported.\n",
    "   - Configure connections to HDFS, Hive, and Kafka correctly.\n",
    "\n",
    "4. **Testing and Validation**:\n",
    "   - Thoroughly test each step with sample data before running the full dataset.\n",
    "   - Validate the output at each stage to ensure accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
